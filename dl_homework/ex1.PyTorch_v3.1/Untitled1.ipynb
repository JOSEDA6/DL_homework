{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3346a273-b51c-4c96-97d9-770cca18a058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\11834/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:18<00:00, 5.40MB/s]\n",
      "C:\\Users\\11834\\AppData\\Local\\Temp\\ipykernel_9776\\1870254077.py:140: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # 混合精度训练[4](@ref)\n",
      "C:\\Users\\11834\\AppData\\Local\\Temp\\ipykernel_9776\\1870254077.py:105: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 145\u001b[0m\n\u001b[0;32m    143\u001b[0m best_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m--> 145\u001b[0m     train(model, device, train_loader, optimizer, scheduler, scaler, epoch)\n\u001b[0;32m    146\u001b[0m     acc \u001b[38;5;241m=\u001b[39m test(model, device, test_loader)\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m acc \u001b[38;5;241m>\u001b[39m best_acc:\n",
      "Cell \u001b[1;32mIn[1], line 107\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, optimizer, scheduler, scaler, epoch)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[0;32m    106\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m--> 107\u001b[0m     loss \u001b[38;5;241m=\u001b[39m lam \u001b[38;5;241m*\u001b[39m criterion(output, targets_a) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lam) \u001b[38;5;241m*\u001b[39m criterion(output, targets_b)\n\u001b[0;32m    109\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    110\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 86\u001b[0m, in \u001b[0;36mLabelSmoothingCrossEntropy.forward\u001b[1;34m(self, pred, target)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pred, target):\n\u001b[1;32m---> 86\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     87\u001b[0m     nll_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mlog_prob\u001b[38;5;241m.\u001b[39mgather(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, index\u001b[38;5;241m=\u001b[39mtarget\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     88\u001b[0m     nll_loss \u001b[38;5;241m=\u001b[39m nll_loss\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# 超参数配置\n",
    "config = {\n",
    "    \"batch_size\": 512,         # 大batch size加速训练[6](@ref)\n",
    "    \"base_lr\": 0.1,            # 初始学习率[6](@ref)\n",
    "    \"weight_decay\": 5e-4,      # L2正则化[6](@ref)\n",
    "    \"momentum\": 0.9,           # Nesterov动量[6](@ref)\n",
    "    \"epochs\": 200,             \n",
    "    \"cutout\": True,            # 高级数据增强[5](@ref)\n",
    "    \"label_smoothing\": 0.1,    # 标签平滑[4](@ref)\n",
    "    \"mixup_alpha\": 0.2         # MixUp增强[4](@ref)\n",
    "}\n",
    "\n",
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 数据增强策略（整合网页[1,5,8](@ref)）\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4)], p=0.8),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3))  # Cutout增强[5](@ref)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# 加载数据集（网页[2,5](@ref)）\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(testset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# 模型定义（基于网页[6,7](@ref)的预训练策略改进）\n",
    "class CifarResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 加载预训练ResNet-50并修改输入层[7](@ref)\n",
    "        self.base_model = torchvision.models.resnet50(pretrained=True)\n",
    "        self.base_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)  # 适配32x32输入[5](@ref)\n",
    "        self.base_model.maxpool = nn.Identity()  # 移除原最大池化层[5](@ref)\n",
    "        \n",
    "        # 修改全连接层[6](@ref)\n",
    "        in_features = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Linear(in_features, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "model = CifarResNet().to(device)\n",
    "\n",
    "# 混合样本增强函数（网页[4](@ref)）\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = torch.distributions.beta.Beta(alpha, alpha).sample().item()\n",
    "    else:\n",
    "        lam = 1.0\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# 损失函数（带标签平滑）\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        log_prob = F.log_softmax(pred, dim=-1)\n",
    "        nll_loss = -log_prob.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -log_prob.mean(dim=-1)\n",
    "        loss = (1.0 - self.smoothing) * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "# 训练函数（整合网页[4,6,8](@ref)优化策略）\n",
    "def train(model, device, train_loader, optimizer, scheduler, scaler, epoch):\n",
    "    model.train()\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=config[\"label_smoothing\"])\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # 应用MixUp增强[4](@ref)\n",
    "        data, targets_a, targets_b, lam = mixup_data(data, target, config[\"mixup_alpha\"])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            output = model(data)\n",
    "            loss = lam * criterion(output, targets_a) + (1 - lam) * criterion(output, targets_b)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # 梯度裁剪[6](@ref)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "# 测试函数\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "# 初始化优化器（网页[6](@ref)配置）\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                     lr=config[\"base_lr\"],\n",
    "                     momentum=config[\"momentum\"],\n",
    "                     weight_decay=config[\"weight_decay\"],\n",
    "                     nesterov=True)\n",
    "\n",
    "# 学习率调度器（网页[6](@ref)余弦退火）\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=config[\"epochs\"])\n",
    "scaler = GradScaler()  # 混合精度训练[4](@ref)\n",
    "\n",
    "# 训练循环\n",
    "best_acc = 0.0\n",
    "for epoch in range(config[\"epochs\"]):\n",
    "    train(model, device, train_loader, optimizer, scheduler, scaler, epoch)\n",
    "    acc = test(model, device, test_loader)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{config['epochs']} | Test Acc: {acc:.2f}%\")\n",
    "\n",
    "print(f\"\\nBest Test Accuracy: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811826d4-d9be-452f-b4b1-d30f3dce217c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
